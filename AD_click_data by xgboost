import gc
import time
import numpy as np
import pandas as pd
from sklearn.cross_validation import train_test_split
import xgboost as xgb
from xgboost import plot_importance
import matplotlib.pyplot as plt
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import LabelEncoder


start = time.time()

dtypes = {
        'ip'            : 'uint32',
        'app'           : 'uint16',
        'device'        : 'uint16',
        'os'            : 'uint16',
        'channel'       : 'uint16',
        'is_attributed' : 'uint8',
        'click_id'      : 'uint32'
        }

# Read the last lines because they are more closed to truth than the starting lines
train = pd.read_csv("../input/train.csv", skiprows=range(1,123903891), nrows=61000000, usecols=['ip', 'app', 'device', 'os', 'channel', 'click_time', 'is_attributed'], dtype=dtypes)

print('Token {} min {} sec to load data'.format(int((time.time()-start)/60),round((time.time()-start)%60),0))

# Make some new features with 'click_time' 
train['hour'] = pd.to_datetime(train.click_time).dt.hour.astype('uint8')
train['day'] = pd.to_datetime(train.click_time).dt.day.astype('uint8')

# Get the columns for target
y = train['is_attributed']
train.drop(['is_attributed'], axis=1, inplace=True)
gc.collect()

# Count the number of clicks by ip-day-hour
gp = train[['ip','day','hour','channel']].groupby(by=['ip','day','hour'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'qty'})
train = pd.merge(train, gp, on=['ip','day','hour'], how='left')
del gp
gc.collect()

# Count the number of clicks by ip-app
gp = train[['ip','app', 'channel']].groupby(by=['ip', 'app'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_app_count'})
train = pd.merge(train, gp, on=['ip','app'], how='left')
del gp
gc.collect()

# Count the number of clicks by ip-app-os
gp = train[['ip','app', 'os', 'channel']].groupby(by=['ip', 'app', 'os'])[['channel']].count().reset_index().rename(index=str, columns={'channel': 'ip_app_os_count'})
train = pd.merge(train, gp, on=['ip','app','os'], how='left')
del gp
gc.collect()

# Optimized data type
train['qty'] = train['qty'].astype('uint16')
train['ip_app_count'] = train['ip_app_count'].astype('uint16')
train['ip_app_os_count'] = train['ip_app_os_count'].astype('uint16')

# Encode label
train[['app','device','os', 'channel', 'hour', 'day']].apply(LabelEncoder().fit_transform)

train.drop(['click_time','ip'],1,inplace=True)
gc.collect()

print('Token {} min {} sec to clean data'.format(int((time.time()-start)/60),round((time.time()-start)%60),0))
print('Start XGBoost Training')

# Set the params(this params from Pranav kernel) for xgboost model
params = {'eta': 0.3,
          'tree_method': "hist",                                  # Fast histogram optimized approximate greedy algorithm.
          'grow_policy': "lossguide",                             # split at nodes with highest loss change
          'max_leaves': 1400,                                     # Maximum number of nodes to be added. (for lossguide grow policy)
          'max_depth': 0,                                         # 0 means no limit (useful only for depth wise grow policy)
          'subsample': 0.9, 
          'colsample_bytree': 0.7, 
          'colsample_bylevel':0.7,
          'min_child_weight':0,                                   # 0 means no limit (useful only for depth wise grow policy)
          'alpha':4,                                              # L1 regularization on weights | default=0 | large value == more conservative model
          'objective': 'binary:logistic', 
          'scale_pos_weight':9,                                   # because training data is extremely unbalanced
          'eval_metric': 'auc', 
          'nthread':8,
          'random_state': 99, 
          'silent': True}
          


# Get 20% of train dataset to use as validation
x1, x2, y1, y2 = train_test_split(train, y, test_size=0.2, random_state=99)
dtrain = xgb.DMatrix(x1, y1)
dvalid = xgb.DMatrix(x2, y2)
del x1, y1, x2 
gc.collect()
watchlist = [(dtrain, 'train'), (dvalid, 'valid')]
model = xgb.train(params, dtrain, 200, watchlist, maximize=True, early_stopping_rounds = 25, verbose_eval=5)

del dtrain
gc.collect()

y_hat = model.predict(dvalid)

del dvalid
gc.collect()

y_hat[y_hat > 0.5] = 1
y_hat[~(y_hat > 0.5)] = 0
print('accuracy:{}'.format(accuracy_score(y2,y_hat)))

del y2,y_hat
gc.collect()

print('Take {} min {} sec  to finish XGBoost Training'.format(int((time.time()-start)/60),round((time.time()-start)%60),0))

# Plot the feature importance from xgboost
plot_importance(model)
plt.gcf().savefig('feature_importance_xgb.png')
